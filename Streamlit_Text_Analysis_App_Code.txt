
# Streamlit App for Text Analysis and Automation Opportunity Identification

import streamlit as st
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
from sklearn.feature_extraction.text import CountVectorizer
from rake_nltk import Rake
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from collections import Counter
from statistics import mean, stdev
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import base64

# Load NLTK stopwords
nltk.download('punkt')
nltk.download('stopwords')

# Streamlit App
st.title("Text Analysis & Automation Opportunity Identification App")

# File upload
uploaded_file = st.file_uploader("Upload your Excel file", type="xlsx")
if uploaded_file is not None:
    data = pd.read_excel(uploaded_file)
    st.write("Data preview:", data.head())

# Preprocessing function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return ' '.join(tokens)

# Analysis options
st.sidebar.title("Analysis Options")
analysis_method = st.sidebar.selectbox("Choose analysis method:", ["RAKE", "KeyBERT", "N-Gram Analysis"])

if analysis_method == "N-Gram Analysis":
    n_value = st.sidebar.slider("Select N for N-Gram:", min_value=2, max_value=6, value=2)

# Threshold options
threshold_method = st.sidebar.selectbox("Choose threshold method:", ["Average + Std Dev", "90th Percentile"])

# Process and analyze data on button click
if st.button("Run Analysis") and uploaded_file is not None:
    # Preprocess text data
    data['Cleaned_Text'] = data['LL_TEXT'].astype(str).apply(clean_text)
    cleaned_texts = data['Cleaned_Text'].tolist()

    if analysis_method == "N-Gram Analysis":
        # N-Gram Analysis
        def get_ngrams(corpus, n):
            vectorizer = CountVectorizer(ngram_range=(n, n))
            ngrams = vectorizer.fit_transform(corpus)
            ngram_freq = dict(zip(vectorizer.get_feature_names_out(), ngrams.sum(axis=0).tolist()[0]))
            return sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)

        ngrams = get_ngrams(cleaned_texts, n_value)
        st.write(f"Top 10 {n_value}-grams:", ngrams[:10])

        # Frequency threshold calculation
        frequencies = [freq for _, freq in ngrams]
        if threshold_method == "Average + Std Dev":
            threshold = mean(frequencies) + stdev(frequencies)
        elif threshold_method == "90th Percentile":
            threshold = np.percentile(frequencies, 90)
        threshold = max(threshold, 5)  # Minimum threshold

        # Filter by threshold
        automation_candidates = [(ngram, freq) for ngram, freq in ngrams if freq > threshold]
        st.write("Automation Candidates:", automation_candidates)

    elif analysis_method == "RAKE":
        # RAKE Keyword Extraction
        rake = Rake()
        data['RAKE_Keywords'] = data['LL_TEXT'].apply(lambda x: rake.extract_keywords_from_text(x) or rake.get_ranked_phrases())
        st.write("RAKE Keywords:", data['RAKE_Keywords'].head())

    elif analysis_method == "KeyBERT":
        # KeyBERT Keyword Extraction
        kw_model = KeyBERT('all-MiniLM-L6-v2')
        data['KeyBERT_Keywords'] = data['LL_TEXT'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=3))
        st.write("KeyBERT Keywords:", data['KeyBERT_Keywords'].head())

    # Clustering
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(cleaned_texts)
    num_clusters = 5
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    labels = kmeans.fit_predict(embeddings)
    data['Cluster'] = labels
    st.write("Clustered Data Preview:", data[['LL_TEXT', 'Cluster']].head())

    # Word Cloud
    all_cleaned_texts = ' '.join(cleaned_texts)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_cleaned_texts)
    st.write("Word Cloud:")
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(plt)

    # Save output to Excel
    output_file = 'processed_text_data_with_analysis.xlsx'
    data.to_excel(output_file, index=False)

    # Download link for output
    def get_download_link(file_path):
        with open(file_path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        href = f'<a href="data:application/octet-stream;base64,{b64}" download="processed_text_data_with_analysis.xlsx">Download Processed File</a>'
        return href

    st.markdown(get_download_link(output_file), unsafe_allow_html=True)
