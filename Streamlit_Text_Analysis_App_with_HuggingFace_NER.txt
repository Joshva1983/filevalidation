
# Streamlit App for Text Analysis, Topic Detection, and NER using Hugging Face Transformers

import streamlit as st
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
from sklearn.feature_extraction.text import CountVectorizer
from rake_nltk import Rake
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from collections import Counter
from statistics import mean, stdev
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import base64
import time
from bertopic import BERTopic
from transformers import pipeline

# Load NLTK stopwords
nltk.download('punkt')
nltk.download('stopwords')

# Load Hugging Face NER model
ner_model = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")

# Streamlit App
st.title("Text Analysis, Topic Detection & NER with Hugging Face Transformers")

# File upload
uploaded_file = st.file_uploader("Upload your file (CSV or Excel)", type=["csv", "xlsx"])
if uploaded_file is not None:
    # Load the file based on its type
    if uploaded_file.name.endswith(".csv"):
        data = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith(".xlsx"):
        data = pd.read_excel(uploaded_file)

    st.write("Data preview:", data.head())

# Preprocessing function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return ' '.join(tokens)

# Sanitize function to remove invalid characters for Excel compatibility
def sanitize_for_excel(text):
    if isinstance(text, str):
        return re.sub(r'[\:*?<>|]', '', text)  # Remove invalid characters
    return text

# Analysis options
st.sidebar.title("Analysis Options")
analysis_method = st.sidebar.selectbox("Choose analysis method:", ["RAKE", "KeyBERT", "N-Gram Analysis", "Topic Detection", "Named Entity Recognition (NER)"])

# N-Gram Range Selection
if analysis_method == "N-Gram Analysis":
    n_value = st.sidebar.slider("Select N for N-Gram:", min_value=2, max_value=6, value=2)

# Threshold options
threshold_method = st.sidebar.selectbox("Choose threshold method:", ["Average + Std Dev", "90th Percentile"])

# Process and analyze data on button click
if st.button("Run Analysis") and uploaded_file is not None:
    # Initialize progress bar
    progress = st.progress(0)
    status_text = st.empty()

    # Step 1: Preprocess text data
    status_text.text("Step 1: Preprocessing data...")
    data['Cleaned_Text'] = data['LL_TEXT'].astype(str).apply(clean_text)
    cleaned_texts = data['Cleaned_Text'].tolist()
    progress.progress(20)
    time.sleep(0.5)

    # Step 2: Run chosen analysis method
    if analysis_method == "N-Gram Analysis":
        status_text.text("Step 2: Performing N-Gram Analysis...")

        # N-Gram Analysis
        def get_ngrams(corpus, n):
            vectorizer = CountVectorizer(ngram_range=(n, n))
            ngrams = vectorizer.fit_transform(corpus)
            ngram_freq = dict(zip(vectorizer.get_feature_names_out(), ngrams.sum(axis=0).tolist()[0]))
            return sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)

        ngrams = get_ngrams(cleaned_texts, n_value)
        st.write(f"Top 10 {n_value}-grams:", ngrams[:10])
        progress.progress(40)
        time.sleep(0.5)

        # Frequency threshold calculation
        status_text.text("Step 3: Calculating frequency threshold...")
        frequencies = [freq for _, freq in ngrams]
        if threshold_method == "Average + Std Dev":
            threshold = mean(frequencies) + stdev(frequencies)
        elif threshold_method == "90th Percentile":
            threshold = np.percentile(frequencies, 90)
        threshold = max(threshold, 5)  # Minimum threshold

        # Filter by threshold
        automation_candidates = [(ngram, freq) for ngram, freq in ngrams if freq > threshold]
        st.write("Automation Candidates:", automation_candidates)
        progress.progress(60)
        time.sleep(0.5)

    elif analysis_method == "RAKE":
        status_text.text("Step 2: Performing RAKE Keyword Extraction...")
        rake = Rake()
        data['RAKE_Keywords'] = data['LL_TEXT'].apply(lambda x: rake.extract_keywords_from_text(x) or rake.get_ranked_phrases())
        st.write("RAKE Keywords:", data['RAKE_Keywords'].head())
        progress.progress(60)
        time.sleep(0.5)

    elif analysis_method == "KeyBERT":
        status_text.text("Step 2: Performing KeyBERT Keyword Extraction...")
        kw_model = KeyBERT('all-MiniLM-L6-v2')
        data['KeyBERT_Keywords'] = data['LL_TEXT'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=3))
        st.write("KeyBERT Keywords:", data['KeyBERT_Keywords'].head())
        progress.progress(60)
        time.sleep(0.5)

    elif analysis_method == "Topic Detection":
        status_text.text("Step 2: Detecting Topics with BERTopic...")
        topic_model = BERTopic()
        topics, probabilities = topic_model.fit_transform(cleaned_texts)
        data['Topic'] = topics
        st.write("Detected Topics Preview:", data[['LL_TEXT', 'Topic']].head())
        st.write("Topic Descriptions:", topic_model.get_topic_info())
        progress.progress(70)
        time.sleep(0.5)

    elif analysis_method == "Named Entity Recognition (NER)":
        status_text.text("Step 2: Performing Named Entity Recognition (NER)...")
        
        # NER using Hugging Face model
        def get_entities(text):
            entities = ner_model(text)
            return [(entity['word'], entity['entity']) for entity in entities]

        data['NER_Entities'] = data['LL_TEXT'].apply(get_entities)
        st.write("NER Results:", data[['LL_TEXT', 'NER_Entities']].head())
        progress.progress(70)
        time.sleep(0.5)

    # Step 4: Clustering
    status_text.text("Step 4: Performing clustering...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(cleaned_texts)
    num_clusters = 5
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    labels = kmeans.fit_predict(embeddings)
    data['Cluster'] = labels
    st.write("Clustered Data Preview:", data[['LL_TEXT', 'Cluster']].head())
    progress.progress(80)
    time.sleep(0.5)

    # Step 5: Generate Word Cloud based on N-Grams if N-Gram Analysis is selected
    status_text.text("Step 5: Generating word cloud...")
    if analysis_method == "N-Gram Analysis":
        wordcloud_text = ' '.join([' '.join([ngram]*freq) for ngram, freq in ngrams])
    else:
        wordcloud_text = ' '.join(cleaned_texts)

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)
    st.write("Word Cloud:")
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(plt)
    progress.progress(90)
    time.sleep(0.5)

    # Step 6: Save and Provide Download Link with Sanitization
    status_text.text("Step 6: Saving processed data and preparing download link...")
    sanitized_data = data.applymap(sanitize_for_excel)
    output_file = 'processed_text_data_with_analysis.xlsx'
    sanitized_data.to_excel(output_file, index=False)
    progress.progress(100)

    # Download link for output
    def get_download_link(file_path):
        with open(file_path, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        href = f'<a href="data:application/octet-stream;base64,{b64}" download="processed_text_data_with_analysis.xlsx">Download Processed File</a>'
        return href

    st.markdown(get_download_link(output_file), unsafe_allow_html=True)
    status_text.text("Analysis completed successfully!")
