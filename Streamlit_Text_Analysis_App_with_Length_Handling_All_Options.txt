
# Streamlit App for Text Analysis with Comprehensive Length Handling

import streamlit as st
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
from sklearn.feature_extraction.text import CountVectorizer
from rake_nltk import Rake
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from collections import Counter
from statistics import mean, stdev
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import base64
import time
from bertopic import BERTopic
from transformers import pipeline

# Load NLTK stopwords
try:
    nltk.download('punkt')
    nltk.download('stopwords')
except Exception as e:
    st.error(f"Error loading NLTK data: {e}")

# Load Hugging Face NER model with error handling
try:
    ner_model = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")
except Exception as e:
    st.error(f"Error loading Hugging Face NER model: {e}")

# Enhanced sanitization function to remove non-printable and non-ASCII characters
def sanitize_for_excel(text):
    if isinstance(text, str):
        # Remove non-ASCII and non-printable characters
        text = re.sub(r'[\:*?<>|]', '', text)  # Remove invalid characters for Excel
        text = ''.join(char for char in text if char.isprintable() and ord(char) < 128)  # Keep only ASCII printable characters
    return text

# Streamlit App Interface
st.title("Text Analysis & Automation Identification App")

try:
    # File upload
    uploaded_file = st.file_uploader("Upload your file (CSV or Excel)", type=["csv", "xlsx"])
    if uploaded_file is not None:
        # Load the file based on its type
        if uploaded_file.name.endswith(".csv"):
            data = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith(".xlsx"):
            data = pd.read_excel(uploaded_file)

        st.write("Data preview:", data.head())

        # Preprocessing function
        def clean_text(text):
            text = text.lower()
            text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
            tokens = word_tokenize(text)
            tokens = [word for word in tokens if word not in stopwords.words('english')]
            return ' '.join(tokens)

        # Analysis options
        st.sidebar.title("Analysis Options")
        analysis_method = st.sidebar.selectbox("Choose analysis method:", ["RAKE", "KeyBERT", "N-Gram Analysis", "Topic Detection", "Named Entity Recognition (NER)"])

        # N-Gram Range Selection for RAKE and KeyBERT
        if analysis_method in ["RAKE", "KeyBERT", "N-Gram Analysis"]:
            min_n_value = st.sidebar.slider("Minimum N for N-Gram:", min_value=1, max_value=6, value=1)
            max_n_value = st.sidebar.slider("Maximum N for N-Gram:", min_value=1, max_value=6, value=2)

        # Process and analyze data on button click
        if st.button("Run Analysis"):
            # Initialize progress bar
            progress = st.progress(0)
            status_text = st.empty()

            # Step 1: Preprocess text data
            status_text.text("Step 1: Preprocessing data...")
            data['Cleaned_Text'] = data['LL_TEXT'].astype(str).apply(clean_text)
            cleaned_texts = data['Cleaned_Text'].tolist()
            progress.progress(20)
            time.sleep(0.5)

            # Step 2: Run chosen analysis method with length handling
            if analysis_method == "RAKE":
                status_text.text("Step 2: Performing RAKE Keyword Extraction...")
                rake = Rake()
                rake_keywords = []

                for text in cleaned_texts:
                    rake.extract_keywords_from_text(text)
                    keywords = rake.get_ranked_phrases() or [""]  # Fill with empty string if no keywords found
                    rake_keywords.append(keywords[0])  # Take the first keyword if multiple

                # Ensure length matches
                while len(rake_keywords) < len(data):
                    rake_keywords.append("")
                data['RAKE_Keywords'] = rake_keywords[:len(data)]
                st.write("RAKE Keywords (Preview):", data['RAKE_Keywords'].head())
                progress.progress(60)
                time.sleep(0.5)

            elif analysis_method == "KeyBERT":
                status_text.text("Step 2: Performing KeyBERT Keyword Extraction...")
                kw_model = KeyBERT('all-MiniLM-L6-v2')
                keybert_keywords = [kw_model.extract_keywords(text, keyphrase_ngram_range=(min_n_value, max_n_value), stop_words='english', top_n=1) for text in cleaned_texts]
                keybert_keywords = [kw[0][0] if kw else "" for kw in keybert_keywords]  # Flatten and handle empty results

                # Ensure length matches
                while len(keybert_keywords) < len(data):
                    keybert_keywords.append("")
                data['KeyBERT_Keywords'] = keybert_keywords[:len(data)]
                st.write("KeyBERT Keywords (Preview):", data['KeyBERT_Keywords'].head())
                progress.progress(60)
                time.sleep(0.5)

            elif analysis_method == "N-Gram Analysis":
                status_text.text("Step 2: Performing N-Gram Analysis...")
                vectorizer = CountVectorizer(ngram_range=(min_n_value, max_n_value))
                ngrams = vectorizer.fit_transform(cleaned_texts)
                ngram_freq = dict(zip(vectorizer.get_feature_names_out(), ngrams.sum(axis=0).tolist()[0]))
                top_ngrams = sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)[:len(data)]
                
                # Pad with empty strings if needed
                ngram_phrases = [ngram for ngram, _ in top_ngrams]
                while len(ngram_phrases) < len(data):
                    ngram_phrases.append("")
                data['Top_NGrams'] = ngram_phrases[:len(data)]
                st.write("Top N-Grams (Preview):", data['Top_NGrams'].head())
                progress.progress(60)
                time.sleep(0.5)

            elif analysis_method == "Topic Detection":
                status_text.text("Step 2: Performing Topic Detection...")
                topic_model = BERTopic()
                topics, _ = topic_model.fit_transform(cleaned_texts)
                
                # Ensure length matches
                while len(topics) < len(data):
                    topics.append("")
                data['Detected_Topics'] = topics[:len(data)]
                st.write("Detected Topics (Preview):", data['Detected_Topics'].head())
                progress.progress(60)
                time.sleep(0.5)

            # Step 6: Save and Provide Download Link with Enhanced Information
            status_text.text("Step 6: Saving processed data and preparing download link...")
            sanitized_data = data.applymap(sanitize_for_excel)
            output_file = 'processed_text_with_comprehensive_output.xlsx'
            sanitized_data.to_excel(output_file, index=False)
            progress.progress(100)

            # Download link for output
            def get_download_link(file_path):
                with open(file_path, "rb") as f:
                    b64 = base64.b64encode(f.read()).decode()
                href = f'<a href="data:application/octet-stream;base64,{b64}" download="processed_text_with_comprehensive_output.xlsx">Download Processed File with Comprehensive Output</a>'
                return href

            st.markdown(get_download_link(output_file), unsafe_allow_html=True)
            status_text.text("Analysis completed successfully!")

except Exception as e:
    st.error(f"An error occurred: {e}")
